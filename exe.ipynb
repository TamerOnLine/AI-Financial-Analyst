{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Import Required Libraries\n",
    "\n",
    "-   This cell imports the necessary libraries: ollama for interacting with the Ollama model and PromptTemplate from LangChain for handling the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Define the Prompt Template\n",
    "\n",
    "-   In this cell, we define the prompt template using LangChain's PromptTemplate. The template ensures that every user query is formatted consistently before sending it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for LangChain\n",
    "template = \"\"\"\n",
    "The user has the following question:\n",
    "Question: {question}?\n",
    "\n",
    "To reach answers that are:\n",
    "    - organized\n",
    "    - deep\n",
    "    - comprehensive\n",
    "\n",
    "The most important thing is that the results are:\n",
    "    - Useful\n",
    "    - Brief\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template with LangChain\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "-   **template**: A formatted text that structures how the user’s question is presented to the model.\n",
    "-   **PromptTemplate**: A function from LangChain used to customize and dynamically insert the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Function to Interact with Ollama Model\n",
    "\n",
    "-   This title conveys that the following function is specifically designed to handle interactions with the Ollama model. It takes user input (questions) and generates appropriate responses by communicating with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with Ollama\n",
    "def chat_with_ollama(question):\n",
    "    \"\"\"\n",
    "    Generate a response from the Ollama model based on the user's question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The response from the Ollama model.\n",
    "    \"\"\"\n",
    "    # Apply the prompt template\n",
    "    prompt = prompt_template.format(question=question)\n",
    "    \n",
    "    # Generate response from Ollama\n",
    "    try:\n",
    "        # Assuming 'ollama.generate' takes the model and prompt as arguments\n",
    "        response = ollama.generate(model=\"llama3.2\", prompt=prompt)\n",
    "        \n",
    "        # Handle the response format, ensuring it's a dictionary\n",
    "        if isinstance(response, dict) and 'response' in response:\n",
    "            return response['response']\n",
    "        else:\n",
    "            return \"Sorry, I couldn't understand the response format.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "This function is a vital component in applications where you need to generate intelligent, context-aware responses based on user input. It could be used in chatbots, virtual assistants, or financial advisory systems that rely on AI models like Ollama to provide real-time responses to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Function to Interact with Ollama\n",
    "\n",
    "-   This cell contains the main function, chat_with_ollama, which formats the user's question using the PromptTemplate and sends it to the Ollama model for generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with Ollama\n",
    "def chat_with_ollama(question):\n",
    "    \"\"\"\n",
    "    Generate a response from the Ollama model based on the user's question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The response from the Ollama model.\n",
    "    \"\"\"\n",
    "    # Apply the prompt template\n",
    "    prompt = prompt_template.format(question=question)\n",
    "    \n",
    "    # Generate response from Ollama\n",
    "    try:\n",
    "        response = ollama.generate(model=\"llama3.2\", prompt=prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "-   chat_with_ollama(question): This function takes the user's query as input, formats it using the LangChain PromptTemplate, and then sends it to the Ollama model to generate a response.\n",
    "-   prompt_template.format(question=question): This formats the question using the predefined prompt template.\n",
    "-   ollama.generate: This method interacts with the Ollama model, sending the formatted query to it.\n",
    "-   try-except: Handles any errors that may occur when calling the Ollama model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Testing the System\n",
    "\n",
    "-   In this cell, you can test the system by asking it a financial-related question, and it will return a response based on the Ollama model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "User: Brief explanation of the meaning of securities and stocks\n",
      "--------------------------------------------------\n",
      "AI Response:\n",
      "Here's a comprehensive, concise, and organized explanation of the meaning of securities and stocks:\n",
      "\n",
      "**Securities:**\n",
      "\n",
      "* Securities refer to financial assets that represent ownership or investment in an entity, such as a company, government, or real estate project.\n",
      "* They are issued by companies, governments, or other organizations to raise capital from investors.\n",
      "* Securities can be debt-based (e.\n",
      "g.\n",
      ", bonds) or equity-based (e.\n",
      "g.\n",
      ", stocks).\n",
      "* The value of securities is determined by the underlying asset's market value and the creditworthiness of the issuer.\n",
      "**Stocks:**\n",
      "\n",
      "* Stocks, also known as equities, represent ownership in a company.\n",
      "* When you buy stocks, you become a shareholder and are entitled to a portion of the company's profits, assets, and liabilities.\n",
      "* Stocks can be traded on stock exchanges, such as the New York Stock Exchange (NYSE) or NASDAQ.\n",
      "* The value of stocks is influenced by various factors, including the company's financial performance, industry trends, and overall market conditions.\n",
      "**Key differences:**\n",
      "\n",
      "* Debt securities (bonds) are obligations to pay interest payments over a fixed period, whereas stock ownership implies long-term involvement in the company's success.\n",
      "* Stocks offer potential for capital appreciation through dividends and share price increases, while debt securities typically yield a fixed interest rate.\n",
      "In summary, securities encompass various types of investments, including stocks, which represent ownership in companies.\n",
      "Understanding both concepts is crucial for making informed investment decisions.\n",
      "\n",
      " mystrotamer\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import NoReturn\n",
    "\n",
    "def chat_chain() -> NoReturn:\n",
    "    \"\"\"\n",
    "    Continuously interact with the user, sending input to Ollama and returning responses.\n",
    "    The chat will end if the user types 'exit'.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Take user input\n",
    "        user_input = input(\"Enter your question (or type 'exit' to quit): \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Generate response from Ollama\n",
    "            response = handle_user_input(user_input)\n",
    "\n",
    "            # Print user input and response in a professional way\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"User: {user_input}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"AI Response:\\n{format_response(response)}\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def handle_user_input(user_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends the user input to Ollama model and returns the response.\n",
    "    \"\"\"\n",
    "    return chat_with_ollama(user_input)\n",
    "\n",
    "def format_response(response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Formats the response in a professional and clean way, adding 'نهاية الرد mystrotamer' at the end.\n",
    "    \"\"\"\n",
    "    if 'response' in response:\n",
    "        result = response['response'].strip()\n",
    "        \n",
    "        # Split into sentences for formatting\n",
    "        sentences = result.split(\".\")\n",
    "        \n",
    "        formatted_response = \"\"\n",
    "        for sentence in sentences:\n",
    "            if sentence.strip():  # Avoid empty sentences\n",
    "                formatted_response += f\"{sentence.strip()}.\\n\"\n",
    "        \n",
    "        # Adding 'نهاية الرد mystrotamer' at the end\n",
    "        formatted_response += \"\\n mystrotamer\"\n",
    "        \n",
    "        return formatted_response.strip()\n",
    "\n",
    "# Run the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat_chain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Code: Interactive Chat System with Ollama Model**\n",
    "\n",
    "-   This code implements an interactive chat system that allows a user to input questions and receive responses from the Ollama model. The system continuously engages with the user until the user types \"exit\" to quit the session. Below is a breakdown of the key components and functionality:\n",
    "\n",
    "1. Function chat_chain():\n",
    "\n",
    "-   This is the main function that handles the interaction loop with the user.\n",
    "-   It continuously prompts the user for input using the input() function and processes it.\n",
    "-   If the user types \"exit\" (case-insensitive), the loop breaks, and the chat ends.\n",
    "-   The function sends the user's input to the Ollama model using handle_user_input().\n",
    "-   It formats and displays both the user's input and the model's response in a clean, professional manner, using separators for better readability.\n",
    "\n",
    "2.  Function handle_user_input(user_input: str) -> str:\n",
    "\n",
    "-   This function sends the user's input to the Ollama model and retrieves the model's response.\n",
    "It abstracts the logic for communicating with the model, keeping the chat_chain() function clean and focused on interaction.\n",
    "\n",
    "3.  Function format_response(response: dict) -> str:\n",
    "\n",
    "-   This function processes and formats the response from the Ollama model.\n",
    "-   It takes the raw response (assumed to be in dictionary format) and strips any unnecessary whitespace.\n",
    "-   The response text is split into sentences, and each sentence is formatted on a new line for improved readability.\n",
    "-   Additionally, a signature (\"mystrotamer\") is appended to the end of the response to indicate the completion of the AI's reply.\n",
    "\n",
    "4.  Exception Handling:\n",
    "\n",
    "The chat_chain() function includes a try-except block to catch any unexpected errors during the interaction.\n",
    "If an error occurs, it is printed for debugging purposes, ensuring the system doesn’t crash abruptly.\n",
    "\n",
    "5.  Execution:\n",
    "\n",
    "-   The script is set to run the chat_chain() function when executed directly (if __name__ == \"__main__\":), ensuring that the chat system is only launched when intended.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "-   Continuous Interaction: The chat system remains active until the user explicitly exits, allowing for multiple queries and responses in a single session.\n",
    "-   Professional Output: The interaction is formatted with clear separators and visual structure, making it user-friendly and easy to follow.\n",
    "-   Extensibility: The code is designed in a modular fashion, making it easy to extend or modify (e.g., by adding new models or response formatting features).\n",
    "-   Error Handling: Built-in error handling ensures that unexpected issues are captured and reported gracefully.\n",
    "\n",
    "**Potential Use Case:**\n",
    "\n",
    "This system is suitable for applications requiring real-time conversational interfaces with AI models, such as virtual assistants, customer support bots, or financial advisory systems, where users can ask questions related to finance or other domains and receive automated, intelligent responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".task1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
